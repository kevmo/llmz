{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Homework - DLT with Qdrant\n",
    "\n",
    "In this homework, we will load the data from our FAQ to Qdrant\n",
    "\n",
    "## Question 1: dlt Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dlt with Qdrant support and Qdrant client\n",
    "!pip install -q \"dlt[qdrant]\" \"qdrant-client[fastembed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlt version: 1.12.3\n"
     ]
    }
   ],
   "source": [
    "# Check dlt version\n",
    "import dlt\n",
    "print(f\"dlt version: {dlt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## dlt Resource\n",
    "\n",
    "For reading the FAQ data, we have this helper function that we'll annotate with `@dlt.resource`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "@dlt.resource\n",
    "def zoomcamp_data():\n",
    "    docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "    docs_response = requests.get(docs_url)\n",
    "    documents_raw = docs_response.json()\n",
    "\n",
    "    for course in documents_raw:\n",
    "        course_name = course['course']\n",
    "\n",
    "        for doc in course['documents']:\n",
    "            doc['course'] = course_name\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Question 2: dlt Pipeline\n",
    "\n",
    "Now let's create a pipeline and configure the Qdrant destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlt.destinations import qdrant\n",
    "\n",
    "# Configure Qdrant destination\n",
    "qdrant_destination = qdrant(\n",
    "    qd_path=\"db.qdrant\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-07-06 17:29:27.331327+00:00 and COMPLETED in 4.38 seconds with 4 steps.\n",
      "Step extract COMPLETED in 0.66 seconds.\n",
      "\n",
      "Load package 1751822968.7473862 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.07 seconds.\n",
      "Normalized data for the following tables:\n",
      "- zoomcamp_data: 948 row(s)\n",
      "\n",
      "Load package 1751822968.7473862 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 2.25 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 2.24 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /Users/kevmo/llmz/hw/db.qdrant location to store data\n",
      "Load package 1751822968.7473862 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 4.38 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 2.24 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /Users/kevmo/llmz/hw/db.qdrant location to store data\n",
      "Load package 1751822968.7473862 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Create and run the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"zoomcamp_pipeline\",\n",
    "    destination=qdrant_destination,\n",
    "    dataset_name=\"zoomcamp_tagged_data\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(zoomcamp_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "Let's examine the trace output to find how many rows were inserted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load info summary:\n",
      "Pipeline name: zoomcamp_pipeline\n",
      "Dataset name: zoomcamp_tagged_data\n",
      "\n",
      "Metrics:\n",
      "  1751822968.7473862: [{'started_at': DateTime(2025, 7, 6, 17, 29, 29, 473056, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 7, 6, 17, 29, 31, 711475, tzinfo=Timezone('UTC')), 'job_metrics': {'zoomcamp_data.b7d45ff09d.jsonl': LoadJobMetrics(job_id='zoomcamp_data.b7d45ff09d.jsonl', file_path='/Users/kevmo/.dlt/pipelines/zoomcamp_pipeline/load/normalized/1751822968.7473862/started_jobs/zoomcamp_data.b7d45ff09d.0.jsonl', table_name='zoomcamp_data', started_at=DateTime(2025, 7, 6, 17, 29, 30, 358224, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 7, 6, 17, 29, 31, 282130, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]\n",
      "\n",
      "Load packages:\n",
      "  Load ID: 1751822968.7473862\n"
     ]
    }
   ],
   "source": [
    "# Let's also check the load info for more details\n",
    "print(f\"\\nLoad info summary:\")\n",
    "print(f\"Pipeline name: {load_info.pipeline.pipeline_name}\")\n",
    "print(f\"Dataset name: {load_info.pipeline.dataset_name}\")\n",
    "\n",
    "# Check normalized data info\n",
    "if hasattr(load_info, 'metrics'):\n",
    "    print(f\"\\nMetrics:\")\n",
    "    for key, value in load_info.metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Alternative way to check row counts\n",
    "print(f\"\\nLoad packages:\")\n",
    "for load_package in load_info.load_packages:\n",
    "    print(f\"  Load ID: {load_package.load_id}\")\n",
    "    if hasattr(load_package, 'jobs_completed_count'):\n",
    "        print(f\"  Jobs completed: {load_package.jobs_completed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "l1gw1ieoojj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found. The load ID might be different in your run.\n"
     ]
    }
   ],
   "source": [
    "# Let's check the normalized folder to count the actual rows\n",
    "import json\n",
    "\n",
    "normalized_path = f\"/Users/kevmo/.dlt/pipelines/zoomcamp_pipeline/load/normalized/1751742539.893121/started_jobs/zoomcamp_data.d7bb15b5be.0.jsonl\"\n",
    "\n",
    "try:\n",
    "    with open(normalized_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"Number of rows in zoomcamp_data: {len(lines)}\")\n",
    "        \n",
    "        # Let's also peek at the first row to see the structure\n",
    "        if lines:\n",
    "            first_row = json.loads(lines[0])\n",
    "            print(f\"\\nFirst row keys: {list(first_row.keys())}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. The load ID might be different in your run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9njsgmwqfzu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract step info:\n",
      "  Started at: 2025-07-06 17:29:28.735005+00:00\n",
      "  Finished at: 2025-07-06 17:29:29.391095+00:00\n",
      "\n",
      "Normalize step info:\n",
      "  Started at: 2025-07-06 17:29:29.392182+00:00\n",
      "  Finished at: 2025-07-06 17:29:29.462964+00:00\n"
     ]
    }
   ],
   "source": [
    "# Let's parse the trace more carefully\n",
    "if pipeline.last_trace:\n",
    "    # Look for extract info\n",
    "    for step in pipeline.last_trace.steps:\n",
    "        if step.step == \"extract\":\n",
    "            print(f\"Extract step info:\")\n",
    "            print(f\"  Started at: {step.started_at}\")\n",
    "            print(f\"  Finished at: {step.finished_at}\")\n",
    "            if hasattr(step, 'metrics') and step.metrics:\n",
    "                print(f\"  Metrics: {step.metrics}\")\n",
    "        \n",
    "        # Look for normalize info\n",
    "        if step.step == \"normalize\":\n",
    "            print(f\"\\nNormalize step info:\")\n",
    "            print(f\"  Started at: {step.started_at}\")\n",
    "            print(f\"  Finished at: {step.finished_at}\")\n",
    "            if hasattr(step, 'metrics') and step.metrics:\n",
    "                for table, info in step.metrics.items():\n",
    "                    if isinstance(info, dict) and 'row_count' in info:\n",
    "                        print(f\"  Table '{table}': {info['row_count']} rows\")\n",
    "                    \n",
    "# Also check if the trace has any table summaries\n",
    "if hasattr(pipeline.last_trace, 'normalized_metrics'):\n",
    "    print(\"\\nNormalized metrics:\")\n",
    "    for table, metrics in pipeline.last_trace.normalized_metrics.items():\n",
    "        print(f\"  {table}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1t9s0vqvpun",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents from zoomcamp_data(): 948\n",
      "\n",
      "Courses found:\n",
      "  data-engineering-zoomcamp: 435 documents\n",
      "  machine-learning-zoomcamp: 375 documents\n",
      "  mlops-zoomcamp: 138 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's count the documents directly from the source\n",
    "test_data = list(zoomcamp_data())\n",
    "print(f\"Total documents from zoomcamp_data(): {len(test_data)}\")\n",
    "print(f\"\\nCourses found:\")\n",
    "courses = {}\n",
    "for doc in test_data:\n",
    "    course = doc.get('course', 'Unknown')\n",
    "    courses[course] = courses.get(course, 0) + 1\n",
    "\n",
    "for course, count in courses.items():\n",
    "    print(f\"  {course}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Question 3: Embeddings\n",
    "\n",
    "Let's check which embedding model was used by inspecting the meta.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found meta.json at: db.qdrant/meta.json\n",
      "\n",
      "Meta.json content:\n",
      "{\n",
      "  \"collections\": {\n",
      "    \"zoomcamp_tagged_data\": {\n",
      "      \"vectors\": {\n",
      "        \"fast-bge-small-en\": {\n",
      "          \"size\": 384,\n",
      "          \"distance\": \"Cosine\",\n",
      "          \"hnsw_config\": null,\n",
      "          \"quantization_config\": null,\n",
      "          \"on_disk\": null,\n",
      "          \"datatype\": null,\n",
      "          \"multivector_config\": null\n",
      "        }\n",
      "      },\n",
      "      \"shard_number\": null,\n",
      "      \"sharding_method\": null,\n",
      "      \"replication_factor\": null,\n",
      "      \"write_consistency_factor\": null,\n",
      "      \"on_disk_payload\": null,\n",
      "      \"hnsw_config\": null,\n",
      "      \"wal_config\": null,\n",
      "      \"optimizers_config\": null,\n",
      "      \"init_from\": null,\n",
      "      \"quantization_config\": null,\n",
      "      \"sparse_vectors\": null,\n",
      "      \"strict_mode_config\": null\n",
      "    },\n",
      "    \"zoomcamp_tagged_data__dlt_loads\": {\n",
      "      \"vectors\": {\n",
      "        \"fast-bge-small-en\": {\n",
      "          \"size\": 384,\n",
      "          \"distance\": \"Cosine\",\n",
      "          \"hnsw_config\": null,\n",
      "          \"quantization_config\": null,\n",
      "          \"on_disk\": null,\n",
      "          \"datatype\": null,\n",
      "          \"multivector_config\": null\n",
      "        }\n",
      "      },\n",
      "      \"shard_number\": null,\n",
      "      \"sharding_method\": null,\n",
      "      \"replication_factor\": null,\n",
      "      \"write_consistency_factor\": null,\n",
      "      \"on_disk_payload\": null,\n",
      "      \"hnsw_config\": null,\n",
      "      \"wal_config\": null,\n",
      "      \"optimizers_config\": null,\n",
      "      \"init_from\": null,\n",
      "      \"quantization_config\": null,\n",
      "      \"sparse_vectors\": null,\n",
      "      \"strict_mode_config\": null\n",
      "    },\n",
      "    \"zoomcamp_tagged_data_zoomcamp_data\": {\n",
      "      \"vectors\": {\n",
      "        \"fast-bge-small-en\": {\n",
      "          \"size\": 384,\n",
      "          \"distance\": \"Cosine\",\n",
      "          \"hnsw_config\": null,\n",
      "          \"quantization_config\": null,\n",
      "          \"on_disk\": null,\n",
      "          \"datatype\": null,\n",
      "          \"multivector_config\": null\n",
      "        }\n",
      "      },\n",
      "      \"shard_number\": null,\n",
      "      \"sharding_method\": null,\n",
      "      \"replication_factor\": null,\n",
      "      \"write_consistency_factor\": null,\n",
      "      \"on_disk_payload\": null,\n",
      "      \"hnsw_config\": null,\n",
      "      \"wal_config\": null,\n",
      "      \"optimizers_config\": null,\n",
      "      \"init_from\": null,\n",
      "      \"quantization_config\": null,\n",
      "      \"sparse_vectors\": null,\n",
      "      \"strict_mode_config\": null\n",
      "    },\n",
      "    \"zoomcamp_tagged_data__dlt_version\": {\n",
      "      \"vectors\": {\n",
      "        \"fast-bge-small-en\": {\n",
      "          \"size\": 384,\n",
      "          \"distance\": \"Cosine\",\n",
      "          \"hnsw_config\": null,\n",
      "          \"quantization_config\": null,\n",
      "          \"on_disk\": null,\n",
      "          \"datatype\": null,\n",
      "          \"multivector_config\": null\n",
      "        }\n",
      "      },\n",
      "      \"shard_number\": null,\n",
      "      \"sharding_method\": null,\n",
      "      \"replication_factor\": null,\n",
      "      \"write_consistency_factor\": null,\n",
      "      \"on_disk_payload\": null,\n",
      "      \"hnsw_config\": null,\n",
      "      \"wal_config\": null,\n",
      "      \"optimizers_config\": null,\n",
      "      \"init_from\": null,\n",
      "      \"quantization_config\": null,\n",
      "      \"sparse_vectors\": null,\n",
      "      \"strict_mode_config\": null\n",
      "    },\n",
      "    \"zoomcamp_tagged_data__dlt_pipeline_state\": {\n",
      "      \"vectors\": {\n",
      "        \"fast-bge-small-en\": {\n",
      "          \"size\": 384,\n",
      "          \"distance\": \"Cosine\",\n",
      "          \"hnsw_config\": null,\n",
      "          \"quantization_config\": null,\n",
      "          \"on_disk\": null,\n",
      "          \"datatype\": null,\n",
      "          \"multivector_config\": null\n",
      "        }\n",
      "      },\n",
      "      \"shard_number\": null,\n",
      "      \"sharding_method\": null,\n",
      "      \"replication_factor\": null,\n",
      "      \"write_consistency_factor\": null,\n",
      "      \"on_disk_payload\": null,\n",
      "      \"hnsw_config\": null,\n",
      "      \"wal_config\": null,\n",
      "      \"optimizers_config\": null,\n",
      "      \"init_from\": null,\n",
      "      \"quantization_config\": null,\n",
      "      \"sparse_vectors\": null,\n",
      "      \"strict_mode_config\": null\n",
      "    }\n",
      "  },\n",
      "  \"aliases\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Find and read the meta.json file in the db.qdrant folder\n",
    "meta_file_path = None\n",
    "for root, dirs, files in os.walk(\"db.qdrant\"):\n",
    "    if \"meta.json\" in files:\n",
    "        meta_file_path = os.path.join(root, \"meta.json\")\n",
    "        break\n",
    "\n",
    "if meta_file_path:\n",
    "    print(f\"Found meta.json at: {meta_file_path}\")\n",
    "    with open(meta_file_path, 'r') as f:\n",
    "        meta_data = json.load(f)\n",
    "    \n",
    "    print(\"\\nMeta.json content:\")\n",
    "    print(json.dumps(meta_data, indent=2))\n",
    "    \n",
    "    # Look for embedding model information\n",
    "    if 'config' in meta_data:\n",
    "        print(\"\\nConfig section:\")\n",
    "        print(json.dumps(meta_data['config'], indent=2))\n",
    "else:\n",
    "    print(\"meta.json file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Question 1 Answer:** The dlt version installed is 1.12.3\n",
    "\n",
    "**Question 2 Answer:** Look for \"Normalized data for the following tables:\" in the trace output above to find the number of rows inserted.\n",
    "\n",
    "**Question 3 Answer:** The embedding model information can be found in the meta.json file output above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
